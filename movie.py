# -*- coding: utf-8 -*-
"""Copy of Sentiment_Analysis_of_Movie_Reviews.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y0u-jrTCy0LYcE0u-_Kej4uc59BA_6u4
"""

import kagglehub

path = kagglehub.dataset_download("lakshmi25npathi/imdb-dataset-of-50k-movie-reviews")
from gensim.models import Word2Vec
import pandas as pd
import numpy as np
import re
import os
import csv

# 1. Download/Path setup
path = kagglehub.dataset_download("lakshmi25npathi/imdb-dataset-of-50k-movie-reviews")
file_path = os.path.join(path, "IMDB Dataset.csv")

# 2. BETTER WAY TO READ: Use quoting=csv.QUOTE_MINIMAL (default)
# but use the 'python' engine which is better at handling messy quotes.
try:
    sentences = pd.read_csv(file_path, engine='python', on_bad_lines='skip')
except Exception as e:
    # If it still fails, this is the "Nuclear Option" that won't mangle your commas:
    sentences = pd.read_csv(file_path, quotechar='"', error_bad_lines=False)

# 3. CRITICAL CHECK: See how many reviews actually loaded
print(f"Loaded {len(sentences)} rows.")
# If this number is much lower than 50,000, that's why your accuracy is bad.

sentences = sentences.dropna(subset=['review'])

# ----------------------
# Preprocessing
# ----------------------
def tokenize(text):
    text = text.lower()
    text = re.sub(r"[^a-z\s]", "", text)
    return text.split()

corpus = [tokenize(str(text)) for text in sentences['review']]

# ----------------------
# Train Word2Vec
# ----------------------
model = Word2Vec(corpus, vector_size=50, window=4, min_count=1, sg=1)

# ----------------------
# Sentence vector
# ----------------------
def sentence_vector(tokens):
    vecs = [model.wv[w] for w in tokens if w in model.wv]
    if not vecs:
        return np.zeros(model.vector_size)
    return np.mean(vecs, axis=0)

from sklearn.model_selection import train_test_split

# 3. Create your X and y arrays
X = np.array([sentence_vector(tokens) for tokens in corpus])

# For y, simply grab the 'sentiment' column and convert to an array
# (No loop needed, which is much faster!)
y = np.array(sentences['sentiment'])

# Perform train-test split, also splitting the original review texts
X_train, X_test, y_train, y_test, reviews_train, reviews_test = train_test_split(X, y, sentences['review'].values, test_size=0.2, random_state=42)

# ----------------------
# Simple sentiment prototypes
# ----------------------
pos_vec = np.mean(X[y == 'positive'], axis=0)
neg_vec = np.mean(X[y == 'negative'], axis=0)

def cosine(a, b):
    norm_a = np.linalg.norm(a)
    norm_b = np.linalg.norm(b)

    # If either vector is all zeros, the similarity is 0
    if norm_a == 0 or norm_b == 0:
        return 0.0

    return np.dot(a, b) / (norm_a * norm_b)

def predict(sentence):
    v = sentence_vector(tokenize(sentence))
    return "positive" if cosine(v, pos_vec) > cosine(v, neg_vec) else "negative"

# ----------------------
# Try it
# ----------------------
tests = [
    "great acting and wonderful story",
    "painfully slow and boring",
    "not bad but not great",
    "I loved the visuals",
    "this movie was atrocious",

]

for t in tests:
    print(t, "â†’", predict(t))

"""### Model Evaluation"""

from sklearn.metrics import accuracy_score

# Make predictions on the test set using the original review texts from the test set
y_pred = [predict(text) for text in reviews_test]

# Convert y_test to a list of strings for comparison with y_pred
y_test_list = y_test.tolist()

# Calculate accuracy
accuracy = accuracy_score(y_test_list, y_pred)

print(f"Model Accuracy: {accuracy:.4f}")

